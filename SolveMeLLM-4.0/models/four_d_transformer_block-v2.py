"""
4D-Transformer Block MVP (v2)
=============================

ä¿®è®¢å†…å®¹ï¼š
- ç»Ÿä¸€ batch_first = [B, T, C]
- çº¦æŸ mask è¯­ä¹‰ä¿®æ­£ï¼šæ— çº¦æŸæ—¶ä¸å‘ G æ³¨å…¥å™ªå£°ç‰¹å¾
- çŠ¶æ€åˆå§‹åŒ–æ›´æ¸©å’Œï¼Œé¿å…è¿‡å¤§åç½®
- FourD çŠ¶æ€æ›´æ–°åŠ å…¥ LayerNorm æå‡ç¨³å®šæ€§
- è®¾å¤‡/shape ç®¡ç†æ›´æ˜ç¡®ï¼Œå‡å°‘ .to(device) å’Œ transpose æ··ä¹±
"""

import math
import torch
import torch.nn as nn
import torch.nn.functional as F  # ç°åœ¨æ²¡æ€ä¹ˆç”¨ï¼Œå¯æŒ‰éœ€åˆ 

# é»˜è®¤å‚æ•°
D_MODEL = 512
NHEAD = 8
DIM_FEEDFORWARD = 2048
DROPOUT = 0.1
STATE_DIM = 128  # æ¯ä¸ªç»´åº¦ 128 ç»´

# ===== 4D é¢†åŸŸ profileï¼šæ“ä½œæ†é…ç½® =====
DEFAULT_DOMAIN_PROFILES = {
    # é€šç”¨ / é»˜è®¤
    "generic": {
        "S": 1.0,  # Self / ç¨³å®š
        "D": 1.0,  # Desire / æ¢ç´¢
        "G": 1.0,  # Ethic / è§„åˆ™
        "R": 1.0,  # Reflex / çº é”™
    },

    # åŒ»ç–—ï¼šæ›´åœ¨æ„ G/Rï¼ˆè§„åˆ™ + çº é”™ï¼‰ï¼Œä½†å¹³è¡¡å‡†ç¡®ç‡
    "medical": {
        "S": 1.1,  # ä»1.2é™åˆ°1.1ï¼Œç¨å¾®æ”¾æ¾ç¨³å®šæ€§
        "D": 1.1,  # ä»1.0æå‡åˆ°1.1ï¼Œæ›´ç§¯æçš„å­¦ä¹ 
        "G": 1.3,  # ä»1.5é™åˆ°1.3ï¼Œè¿›ä¸€æ­¥æ”¾æ¾è§„åˆ™çº¦æŸ
        "R": 1.2,  # ä»1.4é™åˆ°1.2ï¼Œè¿›ä¸€æ­¥æ”¾æ¾çº é”™
    },

    # æ–‡æ¡ˆ / åˆ›ä½œï¼šå¢å¼º Dï¼Œæ”¾æ¾ G ä¸€ç‚¹
    "creative": {
        "S": 0.9,
        "D": 1.5,
        "G": 0.8,
        "R": 0.9,
    },

    # é‡‘è / é£æ§ï¼šG/R æ‹‰é«˜ï¼ŒS ä¹Ÿåé«˜ï¼ŒD ç¨å¾®å‹ä½
    "finance": {
        "S": 1.3,
        "D": 0.8,
        "G": 1.7,
        "R": 1.6,
    },
}


class FourDStateManager(nn.Module):
    """ç®¡ç† token-level çš„ 4D çŠ¶æ€ï¼Œbatch_first = [B, T, C]"""

    def __init__(self, d_model: int, state_dim: int):
        super().__init__()
        self.d_model = d_model
        self.state_dim = state_dim

        scale = 0.02
        self.S0 = nn.Parameter(torch.randn(state_dim) * scale)
        self.D0 = nn.Parameter(torch.randn(state_dim) * scale)
        self.G0 = nn.Parameter(torch.randn(state_dim) * scale)
        self.R0 = nn.Parameter(torch.randn(state_dim) * scale)

        self.state_updater = FourDStateUpdater(d_model, state_dim)

    def init_states(self, batch_size: int, seq_len: int, device):
        """åˆå§‹åŒ– 4D çŠ¶æ€: [B, T, C]"""

        def expand(v: torch.Tensor) -> torch.Tensor:
            base = v.to(device).view(1, 1, -1)  # [1,1,C]
            return base.expand(batch_size, seq_len, -1).contiguous()

        S = expand(self.S0)
        D = expand(self.D0)
        G = expand(self.G0)
        R = expand(self.R0)
        return (S, D, G, R)

    def update(self, x, four_d_states, constraint_mask=None):
        """åŒ…è£…ä¸€ä¸‹ updater.forward æ–¹ä¾¿è°ƒç”¨"""
        S, D, G, R = four_d_states
        return self.state_updater(x, S, D, G, R, constraint_mask=constraint_mask)


class FourDStateUpdater(nn.Module):
    """4D çŠ¶æ€æ›´æ–°å™¨ï¼Œbatch_first = [B, T, *]"""

    def __init__(self, d_model: int, state_dim: int):
        super().__init__()
        self.d_model = d_model
        self.state_dim = state_dim

        # å…±äº«ç¼–ç 
        shared_dim = d_model // 2
        self.shared_enc = nn.Sequential(
            nn.Linear(d_model + state_dim, shared_dim),
            nn.Tanh(),
        )

        # çº¦æŸç¼–ç ï¼ˆEthic ç»´åº¦ç”¨ï¼‰
        constraint_dim = 32
        self.constraint_enc = nn.Sequential(
            nn.Linear(d_model, constraint_dim),
            nn.ReLU(),
        )
        self.shared_enc_G = nn.Sequential(
            nn.Linear(d_model + state_dim + constraint_dim, shared_dim),
            nn.Tanh(),
        )

        # æ¯ä¸ªç»´åº¦çš„ç¼–ç å™¨
        self.enc_S = nn.Linear(shared_dim, state_dim)
        self.enc_D = nn.Linear(shared_dim, state_dim)
        self.enc_G = nn.Linear(shared_dim, state_dim)
        self.enc_R = nn.Linear(shared_dim, state_dim)

        # é—¨æ§æœºåˆ¶
        gate_hid = 16
        self.gate_base = nn.Linear(state_dim, gate_hid)
        self.cand_base = nn.Linear(state_dim, gate_hid)
        self.gate_proj = nn.Linear(gate_hid, state_dim)
        self.cand_proj = nn.Linear(gate_hid, state_dim)

        # ç¼©æ”¾å› å­ï¼ˆæ ‡é‡ï¼‰
        self.gate_scale = nn.Parameter(torch.ones(1) * 0.5)
        self.cand_scale = nn.Parameter(torch.ones(1) * 0.5)

        # ç¨å¾®ç¨³ä¸€ç‚¹
        self.norm_S = nn.LayerNorm(state_dim)
        self.norm_D = nn.LayerNorm(state_dim)
        self.norm_G = nn.LayerNorm(state_dim)
        self.norm_R = nn.LayerNorm(state_dim)

    def _upd(self, enc: torch.Tensor, prev: torch.Tensor) -> torch.Tensor:
        """é—¨æ§æ›´æ–°ï¼šenc/prev: [B, T, C]"""
        h_gate = self.gate_base(enc)
        h_cand = self.cand_base(enc)
        gate = torch.sigmoid(self.gate_proj(h_gate) * self.gate_scale)
        cand = torch.tanh(self.cand_proj(h_cand) * self.cand_scale)
        new = gate * cand + (1.0 - gate) * prev
        return new

    def forward(self, x, S, D, G, R, constraint_mask=None):
        """
        x: [B, T, d_model]
        S, D, G, R: [B, T, state_dim]
        constraint_mask: [B, T]ï¼Œbool æˆ– 0/1
        """
        # shared encoding
        shared_S = self.shared_enc(torch.cat([x, S], dim=-1))
        shared_D = self.shared_enc(torch.cat([x, D], dim=-1))
        shared_R = self.shared_enc(torch.cat([x, R], dim=-1))

        # Ethic ç»´åº¦ï¼šæœ‰çº¦æŸæ—¶æ³¨å…¥ç‰¹å¾ï¼Œå¦åˆ™ä¸º 0
        raw_constraint = self.constraint_enc(x)  # [B,T,constraint_dim]
        if constraint_mask is not None:
            cm = constraint_mask.to(dtype=raw_constraint.dtype).unsqueeze(-1)  # [B,T,1]
            constraint_feat = raw_constraint * cm
        else:
            constraint_feat = torch.zeros_like(raw_constraint)

        shared_G = self.shared_enc_G(torch.cat([x, G, constraint_feat], dim=-1))

        S_enc = self.enc_S(shared_S)
        D_enc = self.enc_D(shared_D)
        G_enc = self.enc_G(shared_G)
        R_enc = self.enc_R(shared_R)

        S_new = self.norm_S(self._upd(S_enc, S))
        D_new = self.norm_D(self._upd(D_enc, D))
        G_new = self.norm_G(self._upd(G_enc, G))
        R_new = self.norm_R(self._upd(R_enc, R))

        return S_new, D_new, G_new, R_new


class FourDSteering(nn.Module):
    """
    4D é¢†åŸŸæ“ä½œæ†ï¼š
    æ ¹æ® current_domain é‡Œçš„ (S,D,G,R) ç³»æ•°ï¼Œç¼©æ”¾å››ä¸ªæ ‡é‡æƒé‡ã€‚
    """

    def __init__(self, domain_profiles=None, default_domain: str = "generic"):
        super().__init__()
        self.domain_profiles = domain_profiles or DEFAULT_DOMAIN_PROFILES
        if default_domain not in self.domain_profiles:
            raise ValueError(f"Unknown default_domain: {default_domain}")
        self.current_domain = default_domain

    def set_domain(self, domain_name: str):
        if domain_name not in self.domain_profiles:
            raise ValueError(f"Unknown domain: {domain_name}")
        self.current_domain = domain_name

    def scale_weights(self, wS: torch.Tensor, wD: torch.Tensor,
                      wG: torch.Tensor, wR: torch.Tensor):
        """
        è¾“å…¥ï¼šå››ä¸ªæ ‡é‡å‚æ•°ï¼ˆnn.Parameterï¼‰
        è¾“å‡ºï¼šæŒ‰é¢†åŸŸ profile ç¼©æ”¾åçš„å››ä¸ªæ ‡é‡
        """
        profile = self.domain_profiles[self.current_domain]
        s = profile["S"]
        d = profile["D"]
        g = profile["G"]
        r = profile["R"]
        return wS * s, wD * d, wG * g, wR * r


class FourDBiasGenerator(nn.Module):
    """æŠŠ 4D çŠ¶æ€èåˆæˆä¸€ä¸ª token-level biasï¼ˆæ”¯æŒé¢†åŸŸæ“ä½œæ†ï¼‰"""

    def __init__(
        self,
        d_model: int,
        state_dim: int,
        domain_profiles=None,
        default_domain: str = "generic",
    ):
        super().__init__()
        self.proj_S = nn.Linear(state_dim, d_model)
        self.proj_D = nn.Linear(state_dim, d_model)
        self.proj_G = nn.Linear(state_dim, d_model)
        self.proj_R = nn.Linear(state_dim, d_model)

        # åˆå€¼å¤§æ¦‚è¡¨ç¤º"è½»å¾®åç½®"ï¼Œåç»­å¯å­¦ä¹ 
        self.weight_S = nn.Parameter(torch.tensor(0.3))
        self.weight_D = nn.Parameter(torch.tensor(0.2))
        self.weight_G = nn.Parameter(torch.tensor(0.3))
        self.weight_R = nn.Parameter(torch.tensor(0.2))

        # ğŸ”¥ é¢†åŸŸæ“ä½œæ†
        self.steering = FourDSteering(
            domain_profiles=domain_profiles,
            default_domain=default_domain,
        )

    def set_domain(self, domain_name: str):
        """å¤–éƒ¨è°ƒç”¨ï¼Œç”¨äºåˆ‡æ¢é¢†åŸŸï¼ˆmedical / creative ç­‰ï¼‰"""
        self.steering.set_domain(domain_name)

    def forward(self, four_d_states):
        """
        four_d_states: (S, D, G, R)ï¼Œeach [B, T, state_dim]
        return: [B, T, d_model]
        """
        S, D, G, R = four_d_states
        S_proj = self.proj_S(S)
        D_proj = self.proj_D(D)
        G_proj = self.proj_G(G)
        R_proj = self.proj_R(R)

        # ğŸ”¥ æŒ‰é¢†åŸŸç¼©æ”¾æƒé‡
        wS, wD, wG, wR = self.steering.scale_weights(
            self.weight_S, self.weight_D, self.weight_G, self.weight_R
        )

        bias = wS * S_proj + wD * D_proj + wG * G_proj + wR * R_proj
        return bias


class FourDPlannerHead(nn.Module):
    """
    4D â†’ Path/Planner å¤´
    - è¾“å…¥: (S,D,G,R) å››ä¸ªçŠ¶æ€ï¼Œ[B, T, state_dim]
    - è¾“å‡º: plan å‘é‡ï¼Œ[B, plan_dim]ï¼Œè¡¨ç¤º"è§„åˆ’è°ƒæ•´"æ‘˜è¦
      ï¼ˆæ¯”å¦‚å¯ä»¥æ¥ä¸€ä¸ªåˆ†ç±»å¤´ã€RL policyã€æˆ–è€…ä½ è‡ªå®šä¹‰çš„ action æ¨¡å—ï¼‰
    """

    def __init__(self, state_dim: int, plan_dim: int = 128, pooling: str = "mean"):
        super().__init__()
        assert pooling in ["mean", "last"], "pooling must be 'mean' or 'last'"
        self.pooling = pooling

        in_dim = state_dim * 4  # æ‹¼æ¥ S,D,G,R
        hidden = state_dim * 2

        self.mlp = nn.Sequential(
            nn.Linear(in_dim, hidden),
            nn.Tanh(),
            nn.Linear(hidden, plan_dim),
        )

    def forward(self, four_d_states):
        """
        four_d_states: (S,D,G,R)ï¼Œeach [B, T, state_dim]
        return: plan [B, plan_dim]
        """
        S, D, G, R = four_d_states  # [B,T,C]

        if self.pooling == "mean":
            # æ²¿æ—¶é—´ç»´æ±‚å¹³å‡ â†’ "å…¨å±€è§„åˆ’"
            S_p = S.mean(dim=1)
            D_p = D.mean(dim=1)
            G_p = G.mean(dim=1)
            R_p = R.mean(dim=1)
        else:  # "last"
            # å–æœ€åä¸€ä¸ª token çš„çŠ¶æ€ â†’ "å½“å‰å±€éƒ¨å†³ç­–"
            S_p = S[:, -1, :]
            D_p = D[:, -1, :]
            G_p = G[:, -1, :]
            R_p = R[:, -1, :]

        fused = torch.cat([S_p, D_p, G_p, R_p], dim=-1)  # [B, 4*state_dim]
        plan = self.mlp(fused)  # [B, plan_dim]
        return plan


class FourDTransformerBlock(nn.Module):
    """
    4D-Transformer Blockï¼ˆbatch_first ç‰ˆæœ¬ï¼‰

    x: [B, T, d_model]
    """

    def __init__(
        self,
        d_model=D_MODEL,
        nhead=NHEAD,
        dim_feedforward=DIM_FEEDFORWARD,
        dropout=DROPOUT,
        state_dim=STATE_DIM,
        domain_profiles=None,
        default_domain: str = "generic",
    ):
        super().__init__()
        self.d_model = d_model
        self.nhead = nhead
        self.state_dim = state_dim

        # æ ‡å‡† Transformer ç»„ä»¶ï¼ˆbatch_first=Trueï¼‰
        self.self_attn = nn.MultiheadAttention(
            d_model, nhead, dropout=dropout, batch_first=True
        )
        self.feedforward = nn.Sequential(
            nn.Linear(d_model, dim_feedforward),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(dim_feedforward, d_model),
            nn.Dropout(dropout),
        )
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

        # 4D çŠ¶æ€ç®¡ç†
        self.four_d_state_manager = FourDStateManager(d_model, state_dim)
        self.four_d_bias = FourDBiasGenerator(
            d_model,
            state_dim,
            domain_profiles=domain_profiles,
            default_domain=default_domain,
        )

    def set_domain(self, domain_name: str):
        """æŠŠé¢†åŸŸåˆ‡æ¢ä¼ ç»™ bias æ¨¡å—"""
        self.four_d_bias.set_domain(domain_name)

    def forward(self, x, four_d_states=None, constraint_mask=None):
        """
        x: [B, T, d_model]
        four_d_states: (S, D, G, R) each [B, T, state_dim] æˆ– None
        constraint_mask: [B, T] å¯é€‰
        """
        B, T, _ = x.shape

        if four_d_states is None:
            four_d_states = self.four_d_state_manager.init_states(
                B, T, device=x.device
            )

        four_d_states = self.four_d_state_manager.update(
            x, four_d_states, constraint_mask=constraint_mask
        )

        bias = self.four_d_bias(four_d_states)  # [B,T,d_model]
        x_bias = x + bias

        # Self-Attentionï¼ˆæ ‡å‡†è‡ªæ³¨æ„åŠ›ï¼‰
        attn_out, _ = self.self_attn(x_bias, x_bias, x_bias)
        x = x + self.dropout(attn_out)
        x = self.norm1(x)

        # Feedforward
        ff_out = self.feedforward(x + bias)
        x = x + self.dropout(ff_out)
        x = self.norm2(x)

        return x, four_d_states


class PositionalEncoding(nn.Module):
    """æ­£å¼¦ä½ç½®ç¼–ç ï¼Œæ¥å£ä¸º batch_first=[B, T, C]"""

    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)  # [max_len,1]
        div_term = torch.exp(
            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)
        )
        pe = torch.zeros(max_len, 1, d_model)  # [max_len,1,C]
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        # buffer ä¸ç®—åˆ° parameters é‡Œ
        self.register_buffer("pe", pe)

    def forward(self, x):
        """
        x: [B, T, C]
        """
        T = x.size(1)
        x = x + self.pe[:T].transpose(0, 1)  # [1,T,C] å¹¿æ’­åˆ° [B,T,C]
        return self.dropout(x)


class FourDTransformer(nn.Module):
    """
    å®Œæ•´çš„ 4D-Transformer æ¨¡å‹ï¼ˆMVP ç‰ˆæœ¬ï¼Œbatch_firstï¼‰

    ç”¨äºéªŒè¯ 4D-Transformer çš„å¯è¡Œæ€§
    """

    def __init__(
        self,
        vocab_size: int,
        d_model=D_MODEL,
        nhead=NHEAD,
        num_layers: int = 6,
        dim_feedforward=DIM_FEEDFORWARD,
        dropout=DROPOUT,
        state_dim=STATE_DIM,
        domain_profiles=None,
        default_domain: str = "generic",
        planner_dim: int | None = None,   # â­ æ–°å¢ï¼šæ˜¯å¦å¯ç”¨ Planner å¤´
        planner_pooling: str = "mean",    # "mean" æˆ– "last"
    ):
        super().__init__()
        self.d_model = d_model
        self.num_layers = num_layers

        self.domain_profiles = domain_profiles or DEFAULT_DOMAIN_PROFILES
        self.current_domain = default_domain

        # Token embedding
        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_encoding = PositionalEncoding(d_model, dropout)

        # 4D-Transformer Blocks
        self.layers = nn.ModuleList(
            [
                FourDTransformerBlock(
                    d_model,
                    nhead,
                    dim_feedforward,
                    dropout,
                    state_dim,
                    domain_profiles=self.domain_profiles,
                    default_domain=default_domain,
                )
                for _ in range(num_layers)
            ]
        )

        # Output head
        self.output_head = nn.Linear(d_model, vocab_size)

        # â­ å¯é€‰çš„ Planner å¤´
        if planner_dim is not None:
            self.planner_head = FourDPlannerHead(
                state_dim=state_dim,
                plan_dim=planner_dim,
                pooling=planner_pooling,
            )
        else:
            self.planner_head = None

        self.init_weights()

    def set_domain(self, domain_name: str):
        """
        å¤–éƒ¨æ¥å£ï¼šä¸€é”®åˆ‡æ¢æ•´ä¸ªæ¨¡å‹çš„é¢†åŸŸ profile
        """
        if domain_name not in self.domain_profiles:
            raise ValueError(f"Unknown domain: {domain_name}")
        self.current_domain = domain_name
        for layer in self.layers:
            layer.set_domain(domain_name)

    def init_weights(self):
        initrange = 0.1
        self.embedding.weight.data.uniform_(-initrange, initrange)
        self.output_head.bias.data.zero_()
        self.output_head.weight.data.uniform_(-initrange, initrange)

    def forward(
        self,
        src,
        constraint_mask=None,
        return_states: bool = False,
        return_plan: bool = False,
    ):
        """
        src: [B, T] token indices
        constraint_mask: [B, T] å¯é€‰
        return_states:
            - False: åªè¿”å› logitsï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
            - True: åŒæ—¶è¿”å›å››ç»´çŠ¶æ€ (S,D,G,R)
        return_plan:
            - True å¹¶ä¸” model æœ‰ planner_head æ—¶ï¼Œè¿”å› plan å‘é‡
        -------
        è¿”å›ï¼š
          - é»˜è®¤: logits
          - return_states=True / return_plan=True:
                (logits, four_d_states, plan)
                å…¶ä¸­ plan å¯èƒ½ä¸º Noneï¼ˆå¦‚æœæ²¡å¯ç”¨ planner_headï¼‰
        """
        x = self.embedding(src) * math.sqrt(self.d_model)  # [B,T,C]
        x = self.pos_encoding(x)

        four_d_states = None
        for layer in self.layers:
            x, four_d_states = layer(x, four_d_states, constraint_mask)

        logits = self.output_head(x)  # [B,T,V]

        plan = None
        if return_plan and (self.planner_head is not None):
            plan = self.planner_head(four_d_states)  # [B, planner_dim]

        if return_states or return_plan:
            return logits, four_d_states, plan

        return logits


if __name__ == "__main__":
    # ç®€å•è‡ªæµ‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # æµ‹è¯•1: ä¸å¸¦Plannerå¤´ï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
    print("=" * 60)
    print("Test 1: Without Planner Head (backward compatible)")
    print("=" * 60)
    model1 = FourDTransformer(vocab_size=10000, d_model=512, nhead=8, num_layers=6)
    model1.to(device)
    
    total_params1 = sum(p.numel() for p in model1.parameters() if p.requires_grad)
    print(f"[INFO] Total trainable parameters: {total_params1:,}")
    
    batch_size = 2
    seq_len = 10
    src = torch.randint(0, 10000, (batch_size, seq_len), device=device)
    constraint_mask = torch.randint(0, 2, (batch_size, seq_len), device=device).bool()
    
    with torch.no_grad():
        logits1 = model1(src, constraint_mask)
    
    print(f"[INFO] Input shape: {src.shape}")
    print(f"[INFO] Output shape: {logits1.shape}")
    print("[OK] 4D-Transformer without Planner Head works!")
    
    # æµ‹è¯•2: å¸¦Plannerå¤´
    print("\n" + "=" * 60)
    print("Test 2: With Planner Head")
    print("=" * 60)
    model2 = FourDTransformer(
        vocab_size=10000,
        d_model=512,
        nhead=8,
        num_layers=6,
        state_dim=128,
        planner_dim=128,
        planner_pooling="mean",
    )
    model2.to(device)
    
    total_params2 = sum(p.numel() for p in model2.parameters() if p.requires_grad)
    print(f"[INFO] Total trainable parameters: {total_params2:,}")
    print(f"[INFO] Additional parameters from Planner Head: {total_params2 - total_params1:,}")
    
    with torch.no_grad():
        # æµ‹è¯•é»˜è®¤è¡Œä¸ºï¼ˆå…¼å®¹æ—§ä»£ç ï¼‰
        logits2 = model2(src, constraint_mask)
        print(f"[INFO] Default output shape: {logits2.shape}")
        
        # æµ‹è¯•è¿”å›stateså’Œplan
        logits2_full, four_d_states, plan = model2(
            src,
            constraint_mask=constraint_mask,
            return_states=True,
            return_plan=True,
        )
        
        S, D, G, R = four_d_states
        print(f"[INFO] Logits shape: {logits2_full.shape}")
        print(f"[INFO] S shape: {S.shape}")
        print(f"[INFO] D shape: {D.shape}")
        print(f"[INFO] G shape: {G.shape}")
        print(f"[INFO] R shape: {R.shape}")
        print(f"[INFO] Plan shape: {plan.shape}")
        print(f"[INFO] Plan norm: {plan.norm(dim=-1).mean().item():.4f}")
    
    print("[OK] 4D-Transformer with Planner Head works!")
    
    # æµ‹è¯•3: ä¸åŒpoolingç­–ç•¥
    print("\n" + "=" * 60)
    print("Test 3: Different Pooling Strategies")
    print("=" * 60)
    
    model3_mean = FourDTransformer(
        vocab_size=10000,
        d_model=512,
        nhead=8,
        num_layers=6,
        state_dim=128,
        planner_dim=128,
        planner_pooling="mean",
    )
    model3_mean.to(device)
    
    model3_last = FourDTransformer(
        vocab_size=10000,
        d_model=512,
        nhead=8,
        num_layers=6,
        state_dim=128,
        planner_dim=128,
        planner_pooling="last",
    )
    model3_last.to(device)
    
    with torch.no_grad():
        _, _, plan_mean = model3_mean(src, constraint_mask=constraint_mask, return_plan=True)
        _, _, plan_last = model3_last(src, constraint_mask=constraint_mask, return_plan=True)
        
        print(f"[INFO] Plan (mean pooling) norm: {plan_mean.norm(dim=-1).mean().item():.4f}")
        print(f"[INFO] Plan (last pooling) norm: {plan_last.norm(dim=-1).mean().item():.4f}")
        print(f"[INFO] Plan difference: {(plan_mean - plan_last).abs().mean().item():.4f}")
    
    print("[OK] Different pooling strategies work!")
    print("\n" + "=" * 60)
    print("All tests passed! [OK]")
    print("=" * 60)